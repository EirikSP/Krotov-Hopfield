{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. juli\n",
    "\n",
    "Begynte med å jobbe videre på vektendrignene i 06_07_23.ipynb for å få bin-breddene til å minke mot 0 og øke igjen etter 0. Kom dessverre ikke noe videre så gikk tilbake til tidligere kode og skrev mer utfyllende om de ulike vektendringsplottene og hva de betyr/viser.\n",
    "\n",
    "Skrev også videre på en påbegynt kode fra tidligere for deep double descent-kurve men for CNN og ikke MLP-nettverks som jeg prøvde på i går. Koden vises nedenfor. Denne tar lang tid å kjøre så derfor lar jeg den kjøre mens jeg endrer mer på de tidligere kodene og skriver forklaringer. Sjekket double descent koden før jeg skulle dra for dagen og den var enda ikke ferdig med å kjøre så fikk ikke sett hvilket output den ga.\n",
    "\n",
    "I koden 23-24_06_23.ipynb fikk jeg til å plotte vektendringene for vektene med høyest verdi over tid som jeg prøvde på tidligere men ikke fikk til. Det viste seg at vektendringen til de største vektene var mye større enn den gjennomsnittlige vektendringen over alle vektene i hvert lag.\n",
    "\n",
    "I 01-04_07_23.ipynb endret jeg lagstørrelsene fra 28^2, 128, 100, 10 til 28^2, 28^2, 28^2, 10 for å se om det var noen endring i vektfordelingen med endring i lagstørrelsen. Det viste seg at fordelingene så like ut selv om det jo var litt endring i antallet vektendringer, men ikke drastiske endringer som var særlig merkbare.\n",
    "\n",
    "Hadde oppdateringsmøte med Anders. Vi diskuterte ulike fordelingsfunksjoner som muligens kan passe til vektfordelingen mellom to epoker i hvert av lagene som jeg fant i 01-04_07_23.ipynb.\n",
    "\n",
    "For å lettere finne en passende fordelingsfunksjon samlet jeg vektfordelingsfigurene for epoke 0-1, 9-10 og 29-30 i én figur. Dette vises i 08_07_23.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(width *28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the width parameters to test\n",
    "widths = list(range(1, 101))\n",
    "\n",
    "# Define the sample sizes\n",
    "sample_sizes = [12500, 25000, 50000]\n",
    "\n",
    "# Define the label noise percentage\n",
    "label_noise = 0.2\n",
    "\n",
    "# Define the number of training epochs\n",
    "epochs = 20\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize lists to store the results\n",
    "test_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the sample sizes\n",
    "for sample_size in sample_sizes:\n",
    "    # Sample a subset from the training dataset\n",
    "    subset_indices = torch.randperm(len(train_dataset))[:sample_size]\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, subset_indices)\n",
    "\n",
    "    # Add label noise to the subset\n",
    "    num_noisy_labels = int(label_noise * sample_size)\n",
    "    noisy_label_indices = torch.randperm(sample_size)[:num_noisy_labels]\n",
    "    for idx in noisy_label_indices:\n",
    "        train_subset.dataset.targets[idx] = torch.randint(0, 10, (1,))\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize lists to store the test errors for each width\n",
    "    test_errors_sample = []\n",
    "\n",
    "    # Iterate over the widths\n",
    "    for width in widths:\n",
    "        # Create the model\n",
    "        model = CNN(width).to(device)\n",
    "\n",
    "        # Define the criterion and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in trange(epochs):\n",
    "            train(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        # Evaluate the model\n",
    "        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "        test_errors_sample.append(test_loss)\n",
    "\n",
    "    # Store the test errors for the current sample size\n",
    "    test_errors.append(test_errors_sample)\n",
    "\n",
    "# Convert the test errors to numpy arrays for plotting\n",
    "test_errors = np.array(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Deep Double Descent - MNIST')\n",
    "plt.xlabel('CNN Width Parameter')\n",
    "plt.ylabel('Test Error')\n",
    "\n",
    "# Plot the graphs for each sample size\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    plt.plot(widths, test_errors[i], label=f'{sample_size} samples')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
