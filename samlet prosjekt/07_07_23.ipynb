{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. juli\n",
    "\n",
    "Begynte med å jobbe videre på vektendrignene i 06_07_23.ipynb for å få bin-breddene til å minke mot 0 og øke igjen etter 0. Kom dessverre ikke noe videre så gikk tilbake til tidligere kode og skrev mer utfyllende om de ulike vektendringsplottene og hva de betyr/viser.\n",
    "\n",
    "Skrev også videre på en påbegynt kode fra tidligere for deep double descent-kurve men for CNN og ikke MLP-nettverks som jeg prøvde på i går. Koden vises nedenfor. Denne tar lang tid å kjøre så derfor lar jeg den kjøre mens jeg endrer mer på de tidligere kodene og skriver forklaringer. Sjekket double descent koden før jeg skulle dra for dagen og den var enda ikke ferdig med å kjøre så fikk ikke sett hvilket output den ga.\n",
    "\n",
    "I koden 23-24_06_23.ipynb fikk jeg til å plotte vektendringene for vektene med høyest verdi over tid som jeg prøvde på tidligere men ikke fikk til. Det viste seg at vektendringen til de største vektene var mye større enn den gjennomsnittlige vektendringen over alle vektene i hvert lag.\n",
    "\n",
    "I 01-04_07_23.ipynb endret jeg lagstørrelsene fra 28^2, 128, 100, 10 til 28^2, 28^2, 28^2, 10 for å se om det var noen endring i vektfordelingen med endring i lagstørrelsen. Det viste seg at fordelingene så like ut selv om det jo var litt endring i antallet vektendringer, men ikke drastiske endringer som var særlig merkbare.\n",
    "\n",
    "Hadde oppdateringsmøte med Anders. Vi diskuterte ulike fordelingsfunksjoner som muligens kan passe til vektfordelingen mellom to epoker i hvert av lagene som jeg fant i 01-04_07_23.ipynb.\n",
    "\n",
    "For å lettere finne en passende fordelingsfunksjon samlet jeg vektfordelingsfigurene for epoke 0-1, 9-10 og 29-30 i én figur. Dette vises i 08_07_23.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(width, width, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(width *28 * 28, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = torch.relu(self.conv4(x))\n",
    "        x = torch.relu(self.conv5(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "def train(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100.0 * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define the data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the width parameters to test\n",
    "widths = list(range(1, 101))\n",
    "\n",
    "# Define the sample sizes\n",
    "sample_sizes = [12500, 25000, 50000]\n",
    "\n",
    "# Define the label noise percentage\n",
    "label_noise = 0.2\n",
    "\n",
    "# Define the number of training epochs\n",
    "epochs = 20\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize lists to store the results\n",
    "test_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:08<02:33,  8.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rache\\OneDrive - Universitetet i Oslo\\nevrale_nettverk\\nevrale_nettverk\\samlet prosjekt\\07_07_23.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m trange(epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     train(model, train_loader, optimizer, criterion, device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m evaluate(model, test_loader, criterion, device)\n",
      "\u001b[1;32mc:\\Users\\rache\\OneDrive - Universitetet i Oslo\\nevrale_nettverk\\nevrale_nettverk\\samlet prosjekt\\07_07_23.ipynb Cell 6\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m output \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\rache\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\rache\\OneDrive - Universitetet i Oslo\\nevrale_nettverk\\nevrale_nettverk\\samlet prosjekt\\07_07_23.ipynb Cell 6\u001b[0m in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rache/OneDrive%20-%20Universitetet%20i%20Oslo/nevrale_nettverk/nevrale_nettverk/samlet%20prosjekt/07_07_23.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv4(x))\n",
      "File \u001b[1;32mc:\\Users\\rache\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rache\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\rache\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate over the sample sizes\n",
    "for sample_size in sample_sizes:\n",
    "    # Sample a subset from the training dataset\n",
    "    subset_indices = torch.randperm(len(train_dataset))[:sample_size]\n",
    "    train_subset = torch.utils.data.Subset(train_dataset, subset_indices)\n",
    "\n",
    "    # Add label noise to the subset\n",
    "    num_noisy_labels = int(label_noise * sample_size)\n",
    "    noisy_label_indices = torch.randperm(sample_size)[:num_noisy_labels]\n",
    "    for idx in noisy_label_indices:\n",
    "        train_subset.dataset.targets[idx] = torch.randint(0, 10, (1,))\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize lists to store the test errors for each width\n",
    "    test_errors_sample = []\n",
    "\n",
    "    # Iterate over the widths\n",
    "    for width in widths:\n",
    "        # Create the model\n",
    "        model = CNN(width).to(device)\n",
    "\n",
    "        # Define the criterion and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "        # Train the model\n",
    "        for epoch in trange(epochs):\n",
    "            train(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        # Evaluate the model\n",
    "        test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n",
    "        test_errors_sample.append(test_loss)\n",
    "\n",
    "    # Store the test errors for the current sample size\n",
    "    test_errors.append(test_errors_sample)\n",
    "\n",
    "# Convert the test errors to numpy arrays for plotting\n",
    "test_errors = np.array(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.title('Deep Double Descent - MNIST')\n",
    "plt.xlabel('CNN Width Parameter')\n",
    "plt.ylabel('Test Error')\n",
    "\n",
    "# Plot the graphs for each sample size\n",
    "for i, sample_size in enumerate(sample_sizes):\n",
    "    plt.plot(widths, test_errors[i], label=f'{sample_size} samples')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
