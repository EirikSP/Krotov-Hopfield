{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. juni\n",
    "\n",
    "Prøvde å gjenskape double descent grafen fra Nakkiran. Så at der hadde de gjenskapt grafer med CNN, og ikke f.eks RFF som brukt i Belkin. Koden kjørte i en evighet, må finnes en mer effektiv måte å gjenskape dem på? \n",
    "\n",
    "Klarer ikke finne igjen koden jeg brukte til dette nå som jeg samler alt sammen, må ha overskrevet den med en annen kode jeg prøvde for å få frem double descend ved å implementere label noise og plotte test error vs epochs. Denne koden vises nedenfor. Den tar også alt for lang tid å kjøre så jeg har ikke fått testet om den funker som den skal.\n",
    "\n",
    "Hørte at vi kanskje skulle sammenligne tre ulike metoder i slutten av prosjektet, hvor back-propagation er en av dem. Brukte derfor litt tid på å lese mer mer opp i dybden på hvordan metoden funker mens koden for double descent kjørte.\n",
    "\n",
    "Så på parameterendringer ved å endre på størrelsen på nettverket i koden fra 23-24_06_23.ipynb og 28_06_23.ipynb:\n",
    "Å minke batch size gjør at modellen raskere blir accurate og loss blir mindre for CNN, samme skjedde for MLP. Dersom batch_size=1 blir det mange fluktuasjoner i accuracy (epochs=10) men jevnt over høy accuracy. For MLP med flere nevroner i de skjulte lagene minker vektendringen. Vektendringen i CNN øker med minkende batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the width parameter\n",
    "width = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(784, width)\n",
    "        self.layer2 = nn.Linear(width, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.layer1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training parameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "test_errors = []\n",
    "for epoch in trange(num_epochs):\n",
    "    total_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Compute test error\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    test_error = 100 - test_accuracy\n",
    "    test_errors.append(test_error)\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {total_loss:.4f}, Test Error: {test_error:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test error versus epochs\n",
    "epochs = range(1, num_epochs)\n",
    "plt.plot(epochs, test_errors)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Error (%)')\n",
    "plt.title(f'Double Descent Curve for Width={width}')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Backpropagation er en algoritme som brukes for å trene nevrale nettverk. Algoritmen utfører en backward pass fra output til input og justerer modellens parametre for å minke feilen og øke nøyaktigheten til nettverket. Vi antar at vi har et MLP-nettverk med 784 input-noder, to skjulte lag med 16 noder og et output-lag med 10 noder.\n",
    "\n",
    "Siden Loss-funksjonen finner gjennomsnittlig loss per bilde for flere tusenvis av treningsbilder vil hvordan vi justerer vektene og biasene til ett gradient descent steg avhenge av alle bildene. For å forklare backpropagation fokuserer vi først på kun ett bilde. \n",
    "\n",
    "Anta at vi sender inn et bilde av et 2-tall og at nettverket ikke er trent enda. Output-nodene vil derfor gi tilfeldige verdier mellom 0 og 1. Vi kan ikke direkte endre disse output-verdiene da vi kun kan påvirke vektene og biasene. Vi ønsker å øke verdien til den tredje noden (3. siden 0-9) og minke verdien til alle de andre nodene, for om nettverket er fullstendig trent skal den tredje noden ha verdi 1 og resten av nodene skal ha verdi 0. Størrelsen av endringen bør være proporsjonal til hvor langt unna hver nodeverdi er fra den idelle verdien. Husk at hver output-verdi er en vektet sum av alle verdiene i det forrige laget bluss bias som i likningen for vektet sum. For å øke verdien av den tredje noden kan man fra likningen se at man kan øke bias $b$ eller vekten $w_i$. Vektene har ulik påvirkning, hvor vektene fra nodene med størst verdi i forrige lag har størst påvirkning på loss-funksjonen (kan sammenlignes med Hebbs teori i nevrovitenskap). Man bør øke vektene $w_i$ proporsjonalt til nodeverdiene $a_i$ i det forrige laget og proporsjonalt med hvor mye de trenger å endres. Siden man ønsker å minke verdien til de andre nodene i output-laget må man endre vektene til hver av disse tilsvarende. Dette forteller oss dermed hva som må endres i det nest-siste laget og hvordan for å få minst mulig loss. Man får en liste av ønskede endringer og gjør samme prosess for bias og vekter i det siste laget for å finne ønskede endringer til dette laget. Husk at dette kun er hvordan vektene og biasene bør være dersom vi sender inn et bilde av et 2-tall. Man gjør nøyaktig det samme for mange andre bilder for å trene nettverket til å gjenkjenne flere ulike bilder. De gjennomsnittlige endringenetil hver vekt og bias er den negative gradienten til loss-funksjonen.\n",
    "\n",
    "Siden det er ekstremt tidkrevende å finne endringen til hvert eneste treningsbilde og hvert gradient descent steg deler vi datasettet inn i batches og regner ut gradient descent steget til hver batch som gjør prosessen raskere. Denne prosessen kalles stochastic gradient descent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep double descent\n",
    "Deep double descent er et fenomen som oppstår i CNNs hvor prestasjonen først øker, så blir verre og deretter øker igjen med økende modellstørrelse, datastørrelse eller treningstid. Dette fenomenet unngås ofte ved nøye regulering. Man er enda ikke helt sikker på hvorfor det skjer.\n",
    "\n",
    "Double descent-fenomenet ble først oppdaget av Mikhail Belkin som undersøkte påstanden mange hadde om at \"større modeller alltid er bedre\", selv om statisktisk ML-teori forutsa at større modeller hadde større sannsynlighet for overfitting. \n",
    "\n",
    "Bias-variance tradeoff er en egenskap til ML-modeller hvor variansen til et sett med data kan reduseres ved å øke bias. Man ønsker å minimere begge disse kildene til feil. En bias feil er en feil som fører til underfitting, og variansefeil fører til overfitting pga mye tilfeldig støy i treningsdataen.\n",
    "\n",
    "Data- og label-støy er antatte avvik fra det faktiske datasettet. Datastøy er avvik i dataen, altså bildene, mens labelstøy er avvik i labels."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
