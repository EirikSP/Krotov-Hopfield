{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppsummering av hovedfunn fra prosjektet:\n",
    "(20. juli)\n",
    "\n",
    "Hovedmålet med dette sommerprosjektet har vært å undersøke vektendringen som skjer i et nevralt nettverk. Til det har jeg brukt MNIST-datasettet og hovedsakelig MLP-nettverk. \n",
    "\n",
    "Jeg startet med å lese meg opp på nevrale nettverk, da dette var helt ukjent for meg da jeg startet. Etter å ha fått flere nettverkskoder til å kjøre (19-21_06_23.ipynb + 22_06_23.ipynb) og satt meg inn i pytorch sin dokumentasjon fikk jeg laget min egen nettverkskode som jeg bygget videre på (23-24_06_23.ipynb). \n",
    "\n",
    "I denne så jeg på gjennomsnittlig vektendring i hvert av lagene over flere epoker. En epoke er 1 forward pass og 1 backpropagation for alle training-samples. For hver epoke regnet jeg altså ut hvor stor gjennomsnittlig endring det var mellom vektene for hvert av lagene i nettverket. Ved å plotte vektdifferanse mot antall epoker så jeg at vektdifferansen mellom hver epoke minker ettersom antall epoker øker. Dette er som forventet siden et nettverk som er trent over lengre tid vil ikke trenge å endre vektene like mye. Så også at dersom vi har et nettverk med 3 lag vil det siste laget ha størst vektendring mens de første lagene vil ikke ha like stor vektendring. Dette avhenger nok også av størrelsen på lagene. I dette tilfellet brukte jeg 784 input-noder, 100 noder i første skjulte lag, 50 noder i andre skjulte lag og 10 noder som output. Så også på vektfordelingen i de ulike lagene som viste meg at det var flest vekter med verdier nære null. For det første laget hadde nesten alle vektene verdier rundt null, mens med økende skulte lag økte også vektverdien.\n",
    "\n",
    "Etter å ha sett gjennom artikkelen \"Self-Organized Learning in the Chialvo-Bak Model\" av Marco Brigham prøvde jeg å plotte vekter mot node-index for det første skjulte laget og fikk et plott som så likt ut som det i figur 2.5 a) selv om Chialvo-Bak-modellen har en litt annen nettverksoppbygging enn MLP-modellen jeg bruker med backpropagation.\n",
    "\n",
    "Det neste jeg prøvde på var å lage en modell som gradvis trener et nevralt nettverk (26-27_06_23.ipynb). Nettverket trenes først til å kun gjennkjenne bilder av tallene 0-3. Deretter legger jeg inn bilder av 4-tall inn i treningssettet etter 10 epoker og bilder av 5-tall etter 20 epoker. Dette ga meg først et uventet plott med gaussisk form som jeg når jeg skriver dette ser at ejg glemte å undersøke videre. Begynte heller på et nytt script senere i (30_06_23.ipynb) hvor jeg fikk rettet opp på det som var galt og plottene jeg fikk stemte overrens med det jeg hadde forventet som var at vektendringen avtar mot epoke 10 og når 4-tallet legges til i treningssettet vil vektendringen øke igjen, så avta mot 20, få en økning når 5-tallet legges til i treningssettet og så avta igjen. Samme type plott fikk jeg også for riktigheten til modellen, der riktigheten øker jevnt men blir så mindre når man legger til et nytt tall i treningssettet.\n",
    "\n",
    "Så gjennom artiklene \"Reconciling modern machine learning practice\n",
    "and the bias-variance trade-of\" av Mikhail Belkin og \"DEEP DOUBLE DESCENT:\n",
    "WHERE BIGGER MODELS AND MORE DATA HURT\" av Preetum Nakkiran. Siden det brukes nettverk av typen CNN i den ene artikkelen og jeg ønsket å prøve å gjenskape double descent grafene og forstå dem bedre laget jeg en CNN-implementasjon med samme parametere som tidligere. Det viste seg at resultatene jeg får er omtrent like de jeg får med ett MLP-nettverk. Den største forskjellen er tiden det tar å kjøre koden da en MLP-implementering går mye raskere å kjøre grunnet færre computations. CNN-implementasjonen finnes i 28_06_23.ipynb.\n",
    "\n",
    "Prøvde i 29_06_23.ipynb å gjenskape double descent grafen fra Nikkaran-artikkelen ved å legge til label noise og plotte test error vs epochs. Denne tok altfor lang tid å kjøre og vet derfor ikke om den funker eller om den gir noen double descent graf.\n",
    "\n",
    "Så deretter på hvordan parameterendring påvirket riktigheten til modellen ved både accuracy og cross entropy loss og hva det gjorde med vektendringen. Da så jeg at å minke batch size gjør at modellen raskere får høyere riktighet. Dette gjaldt både for nettverk av typen MLP og CNN. Flere epoker ga høyere riktighet, men tok selvsagt lengre tid å kjøre. For MLP ga flere nevroner i de skjulte lagene mindre vektendringer og vektendringen i CNN øker med minkende batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det neste jeg prøvde på var å visualisere vekdifferansen mellom den første og andre epoken i ett av lagene til nettverket. Jeg valgte det første laget til det. Altså lagde jeg et histogram for vektendringen mellom to epoker hvor y-aksen viser hvor mange av de ulike vektendringene som oppstår og x-aksen viser størrelsen på vektendringene. Valgte å se på abosluttverdien av vektendringene og forventet da at de aller fleste vektendringene hadde verdi rundt 0. Derfor prøvde jeg å implementere en økende bin-bredde i histogrammet slik at det ble en mer riktig fremstilling hvor første bin som starter ved 0 ikke blir mye høyere enn de andre, men smalere og kun litt høyere. Det viste seg å ikke være den letteste oppgaven, men jeg fikk det til etterhvert og resultatet finnes i 01-04_07_23.ipynb.\n",
    "\n",
    "Som forventet, så jeg at det var flest vektendringer rundt 0 slik at første bin var den høyeste. Jeg prøvde så å plotte sannsynlighetstettheten sammen med histogrammet og gjorde dette også for lag 2 og 3. (Størrelsen på lagene er input:784, hidden1:784, hidden2:784 og output:10) Det jeg så var da at for lag 1 var det færre vektendringer totalt, men de var av litt større størrelse enn de for lag 2. I lag 2 var det mange flere vektendringer men av mindre størrelse. For lag 3 var det enda færre vektendringer totalt enn for lag 1, noe som skyldes at lag 3 kun har 10 noder. Vektendringene her var også mye større enn de som var for lag 1 og lag 2. \n",
    "\n",
    "Videre plottet jeg histogrammer for vektdifferansen mellom epoke 0-1 som i starten men også for vektdifferansen mellom epoke 9-10 og 29-30. Det jeg så da var at når det hadde gått flere epoker ble det totalt færre og færre vektendringer, men de ble større i størrelsen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mens jeg ventet på å få tilgang til tungregningsmaskinene på insituttet undersøkte jeg t-SNE-visualisering av MNIST-datasettet. En implementering av dette finnes i 05_07_23.ipynb hvor man ser hvordan både treningssettet og testsettet kan deles inn i grupper for de ulike tallbildene etter trening og visuliseres i 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etter jeg fikk tilgang til tungregningsmaskinene prøvde jeg igjen å kjøre koden fra 29_06_23.ipynb for å teste om den ville gi double descent kurve, men selv med tilgang til tungregningsmaskinene tok dette veldig lang tid og før jeg dro hjem for dagen fikk jeg et plott, men det viste ikke noe tegn til double descent dessverre. I etterkant så jeg at jeg hadde implementert et MLP-nettverk og ikke CNN som kan være en av grunnene til at jeg ikke fikk resultatet jeg håpet på, men det er nok flere ting som spiller inn som størrelse på modellen og antall epoker også. Prøvde i 07_07_23.ipynb å bruke CNN, men denne koden tok enda lengre tid å kjøre så den vet jeg ikke hva slags resultat gir. Må nok innse at jeg ikke har nok datakraft til å gjenskape dette fenomenet.\n",
    "\n",
    "Prøvde meg på å se på både de negative og positive vektendringene i 06_07_23.ipynb, men det viste seg å være veldig vanskelig å få til minkende bin-bredder mot 0 fra den negative siden og så økende bin-bredder fra 0 igjen og bestemte meg for å ikke se mer på det da absoluttverdien av vektendringene også gir en god oversikt over vektendringene. Uten endring av bin-breddene ser det ut som om det er omtrent lik fordeling av positive og negative vektendringer.\n",
    "\n",
    "Fant ut av at dersom størrelsen på nettverket økes fra 784, 128, 100, 10 til 784, 784, 784, 10 vil det ikke gi noen særlig merkbar endring i vektfordelingen, men resultatene vil være nokså like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det siste jeg undersøkte i 08+17-19.ipynb var hvilken fordelingsfunksjon som vektendringene i det nevrale nettverket kunne passe. Til det tok jeg utgangspunkt i sannsynlighetsfordelingskurvene jeg lagde i scriptet 01-04_07_23.ipynb. Uten å ha tatt noen emner i statistikk viste dette seg å bli en krevende oppgave. Fra sannsynlighetstetthetskurven gjettet jeg først på en slags eksponetialfordeling, log-fordeling eller en potenslov-fordeling. Derfor begynte jeg med å teste disse fordelingene med ulike variable for å se om noen av de lignet. Jeg plottet de tre sannsynlighetstetthetsgrafene for epoke 0-1, 9-10 og 29-30 i samme figur og så at start og sluttpunktet dems var litt ulikt selv om plottet hver for seg så fordelingene nesten helt identiske ut. For å sjekke om vektendringene hadde en eksponetialfordeling gjorde jeg aksene logaritmiske og så at når y-aksen var logaritmisk, mens x-aksen var lineær ble vektfordelingen tilnærmet lineær noe som tyder på en eksponetialfordeling.\n",
    "\n",
    "Jeg prøvde å lese meg litt opp på metoden jeg fikk forklart av arbeidsgiver med å innføre en karakteristisk størrelse, men etter å ha kvernet på det en stund uten noen resultater bestemte jeg meg for å prøve noen av pythons innebygde biblioteker for data-graf-tilpasning. Jeg prøvde python-biblioteket fitter hvor jeg testet ut noen av de vanligste fordelingsfunksjonene på dataen, men ingen av de så ut til å passe spesielt godt. Denne fremgangsmåten plottet også et histogram og jeg er ikke sikker på hvor godt bin-bredde-endringen ble bevart og prøvde derfor en annen metode fra scipy som heter curve_fit(). Med denne metoden prøvde jeg ut flere ulike fordelignsfunksjoner for alle de ulike epokene og fant til slutt at fordelingsfunksjonene  $f(x, a, b) = ae^{-bx}$ og $f(x, a, b) = ax^{-bx}$ var de som passet best med minst usikkerhet for parameterne $a$ og $b$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
